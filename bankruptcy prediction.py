# -*- coding: utf-8 -*-
"""assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DV10uZFn1s6fs3rBvJCxw9UgjDaR_QPF
"""

#Assignment 1
#Classification problem
#Healthy-Bankrupt companies dataset

#Μηχανική Μάθηση 2024 (εξάμηνο 7ο)
#Καθηγητής: Ευτύχιος Πρωτοπαπαδάκης
#Φοιτητες: iis22127 Δαδακίδης Γιώργος
#          iis22125 Παναγιώτης Μώκος

#LIBRARY TO READ THE DATA FROM EXCEL
!pip install pandas openpyxl
#LIBRARY TO CREATE GRAPHS
!pip install matplotlib seaborn

import pandas as pd
from google.colab import drive
#Read the Excel file from Google Drive

drive.mount('/content/drive')


file_path = '/content/drive/My Drive/Colab Notebooks/Dataset2Use_Assignment1.xlsx'


df = pd.read_excel(file_path)

#FIGURE 1: Healthy/Bankrupt Companies


df = pd.read_excel(file_path)

print(df.shape)
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.lines import Line2D
df.rename(columns={df.columns[11]: 'Bankruptcy_Status', df.columns[12]: 'Year'}, inplace=True)



# Count the number of companies that are healthy and bankrupt for each year
data_grouped = df.groupby(['Year', 'Bankruptcy_Status']).size().reset_index(name='Count')
custom_colors = {1: 'green', 2: 'red'}

# Create a new column to map the status to colors
data_grouped['Color'] = data_grouped['Bankruptcy_Status'].map(custom_colors)
# Use seaborn to create a bar plot
plt.figure(figsize=(14, 6))
sns.barplot(x='Year', y='Count', hue='Bankruptcy_Status', data=data_grouped,palette=custom_colors)

# Adding the actual numbers on top of the bars
# Get the axes object and iterate over the bars
ax = plt.gca()
for p in ax.patches:
    height = p.get_height()
    ax.text(
        p.get_x() + p.get_width() / 2,  # X position: Center of the bar
        height + 1,  # Y position: Slightly above the bar
        f'{int(height)}',  # The text to display
        ha='center',  # Horizontal alignment
        va='bottom',  # Vertical alignment
        color='black'  # Text color
    )

# Customizing the legend to make it clearer
handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10)
           for color in custom_colors.values()]
labels = ['Healthy (1)', 'Bankrupt (2)']
plt.legend(handles, labels, title='Status')
# Customizing the legend to use `custom_colors`
handles = [
    Line2D([0], [0], marker='o', color='w', markerfacecolor=custom_colors[1], markersize=10, label='Healthy (1)'),
    Line2D([0], [0], marker='o', color='w', markerfacecolor=custom_colors[2], markersize=10, label='Bankrupt (2)')
]

plt.legend(handles=handles, title='Status')
# Adding labels and title
plt.xlabel('Year')
plt.ylabel('Number of Companies')
plt.title('Figure1: Number of Healthy and Bankrupt Companies per Year')

# Show the plot
plt.xticks(rotation=45)
plt.show()

#FIGURE2= MIN,MAX,AVG for every indicator
#!!!!!!!!RUN BEFORE THE NORMALIZATION !!!!!!!!!
import pandas as pd
import matplotlib.pyplot as plt
print(df.head(10))


df.rename(columns={df.columns[11]: 'Bankruptcy_Status'}, inplace=True)

# Ensure correct filtering of data
healthy_companies = df[df['Bankruptcy_Status'] == 1]  # Healthy companies (1)
bankrupt_companies = df[df['Bankruptcy_Status'] == 2]  # Bankrupt companies (2)

# Select columns A to H (performance indicators) assuming they are the first 8 columns
indicators_columns = df.columns[:8]

# Loop through each indicator and create a graph
for indicator in indicators_columns:
    # Calculate min, max, and average for both groups for the current indicator
    healthy_stats = healthy_companies[indicator].agg(['min', 'max', 'mean'])
    bankrupt_stats = bankrupt_companies[indicator].agg(['min', 'max', 'mean'])

    # Determine global min and max to set y-axis limits
    global_min = min(healthy_stats.min(), bankrupt_stats.min())
    global_max = max(healthy_stats.max(), bankrupt_stats.max())

    # Create subplots for the current indicator
    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)

    # Healthy Companies Plot
    bars_healthy = axes[0].bar(['Min', 'Max', 'Avg'], healthy_stats, color=['blue', 'red', 'green'])
    axes[0].set_title(f'Healthy Companies - {indicator}')
    axes[0].set_ylabel('Value')

    # Set y-axis to global min and max for both plots
    axes[0].set_ylim(global_min - abs(global_min) * 0.1, global_max * 1.1)

    # Add value labels on top of each bar for Healthy companies
    for bar in bars_healthy:
        yval = bar.get_height()
        axes[0].text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2),
                     ha='center', va='bottom', fontsize=10)

    # Bankrupt Companies Plot
    bars_bankrupt = axes[1].bar(['Min', 'Max', 'Avg'], bankrupt_stats, color=['blue', 'red', 'green'])
    axes[1].set_title(f'Bankrupt Companies - {indicator}')

    # Set y-axis to global min and max for both plots
    axes[1].set_ylim(global_min - abs(global_min) * 0.1, global_max * 1.1)

    # Add value labels on top of each bar for Bankrupt companies
    for bar in bars_bankrupt:
        yval = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2),
                     ha='center', va='bottom', fontsize=10)

    # Adjust layout and show the plots
    plt.tight_layout()
    plt.show()
print(df.head(10))

# Check if there are any missing values


missing_data = df.isnull().sum()


if missing_data.any():
    print("There are missing values in the dataset.")
    print("Here is the count of missing values for each column:")
    print(missing_data[missing_data > 0])  # Display only columns with missing values
else:
    print("No missing values found in the dataset.")

#Normalization using MinMax
from sklearn.preprocessing import MinMaxScaler


# Select all numeric columns, excluding the year column
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns


# Initialize and apply the MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
df[numeric_columns] = scaler.fit_transform(df[numeric_columns])

# Check the result to confirm the year column is unchanged
print(df.head(100))

#Create 4 folds using Stratified kfold
from sklearn.model_selection import StratifiedKFold

# Define the features (X) and target variable (y)
X = df.drop(columns=['Bankruptcy_Status'])  # Features
y = df['Bankruptcy_Status']  # Target variable (1 for healthy, 2 for bankrupt)

# Initialize StratifiedKFold with 4 splits
skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)

# Loop through the folds
for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Get counts and percentages for training set
    train_counts = y_train.value_counts()
    train_percentages = y_train.value_counts(normalize=True) * 100

    # Get counts and percentages for testing set
    test_counts = y_test.value_counts()
    test_percentages = y_test.value_counts(normalize=True) * 100

    print(f"Fold {fold + 1}:")
    print("Training set:")
    print(f"  Healthy companies: {train_counts[0]} ({train_percentages[0]:.2f}%)")
    print(f"  Bankrupt companies: {train_counts[1]} ({train_percentages[1]:.2f}%)")
    print("Testing set:")
    print(f"  Healthy companies: {test_counts[0]} ({test_percentages[0]:.2f}%)")
    print(f"  Bankrupt companies: {test_counts[1]} ({test_percentages[1]:.2f}%)")
    print("-" * 40)

# Identify the normalized values for "Healthy" (1) and "Bankrupt" (2)
normalized_healthy = df['Bankruptcy_Status'].min()  # Normalized value of 1
normalized_bankrupt = df['Bankruptcy_Status'].max()  # Normalized value of 2

# Separate the data by class (Healthy and Bankrupt)
healthy_companies = df[df['Bankruptcy_Status'] == normalized_healthy]
bankrupt_companies = df[df['Bankruptcy_Status'] == normalized_bankrupt]

# Sample 25% of bankrupt companies for the training set
train_bankrupt = bankrupt_companies.sample(frac=0.25, random_state=42)

# Sample enough healthy companies for the training set to get a 75% healthy and 25% bankrupt ratio
train_healthy = healthy_companies.sample(n=int(len(train_bankrupt) * 3), random_state=42)

# Combine to create the training set
train_set = pd.concat([train_healthy, train_bankrupt])

# Create the test set:
# 1. All bankrupt companies not in the training set.
# 2. For healthy companies: Sample only enough to maintain the original 97% healthy to 3% bankrupt ratio.
remaining_bankrupt = bankrupt_companies.drop(train_bankrupt.index)
test_healthy_count = int(len(remaining_bankrupt) * (97 / 3))  # Maintain 97:3 ratio
remaining_healthy = healthy_companies.drop(train_healthy.index).sample(n=test_healthy_count, random_state=42)

# Combine to create the test set
test_set = pd.concat([remaining_healthy, remaining_bankrupt])

# Check the new distributions in the training and test sets
train_distribution = train_set['Bankruptcy_Status'].value_counts(normalize=True) * 100
test_distribution = test_set['Bankruptcy_Status'].value_counts(normalize=True) * 100

# Print the distributions
print("Training set distribution:")
print(f"  Healthy companies: {train_set['Bankruptcy_Status'].value_counts()[normalized_healthy]} ({train_distribution[normalized_healthy]:.2f}%)")
print(f"  Bankrupt companies: {train_set['Bankruptcy_Status'].value_counts()[normalized_bankrupt]} ({train_distribution[normalized_bankrupt]:.2f}%)")

print("\nTest set distribution:")
print(f"  Healthy companies: {test_set['Bankruptcy_Status'].value_counts()[normalized_healthy]} ({test_distribution[normalized_healthy]:.2f}%)")
print(f"  Bankrupt companies: {test_set['Bankruptcy_Status'].value_counts()[normalized_bankrupt]} ({test_distribution[normalized_bankrupt]:.2f}%)")

################################################################################
# COMPLETE CODE CELL: TRAIN/TUNE MULTIPLE CLASSIFIERS + PRINT & PLOT EVERYTHING
################################################################################
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time, os, csv

from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, RocCurveDisplay
)

# Import estimators
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB

from scipy.stats import randint, uniform

###############################################################################
# 1) DATA, FOLDS, AND BOTH "results_df" + "fold_metrics"
###############################################################################
# Suppose X_balanced, y_balanced are your balanced dataset features & labels
# e.g.:
X_balanced = train_set.drop(columns=['Bankruptcy_Status'])
y_balanced = train_set['Bankruptcy_Status']
n_splits = 4
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# This DataFrame logs row-by-row results for each fold & each classifier
results_df = pd.DataFrame(columns=[
    "Classifier Name", "Fold", "TrainOrTest", "BalancedOrUnbalanced",
    "Num Train Samples", "Num Non-Healthy in Train",
    "TP", "TN", "FP", "FN", "ROC-AUC"
])

# This dictionary logs aggregated metrics (Accuracy, Precision, etc.) across folds
fold_metrics = {
    "Classifier": [],
    "Fold": [],
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1 Score": [],
    "AUC ROC": []
}

# If you want to label your dataset as "balanced" or "unbalanced"
balanced_status = "balanced"

###############################################################################
# 2) HELPER FUNCTION: log_metrics_to_df
###############################################################################
def log_metrics_to_df(
    df, classifier_name, fold_number, dataset_type, balanced_status,
    y_true, y_pred, X_data, y_data, y_prob
):
    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    TN, FP, FN, TP = cm.ravel()

    # Calculate ROC-AUC if probabilities are provided
    roc_auc = None
    if y_prob is not None:
        roc_auc = roc_auc_score(y_true, y_prob)

    # Construct a row dictionary
    row = {
        "Classifier Name": classifier_name,
        "Fold": fold_number,
        "TrainOrTest": dataset_type,
        "BalancedOrUnbalanced": balanced_status,
        "Num Train Samples": len(X_data),
        "Num Non-Healthy in Train": sum(y_data == 1),
        "TP": TP,
        "TN": TN,
        "FP": FP,
        "FN": FN,
        "ROC-AUC": roc_auc
    }

    # Convert the row to a DataFrame and concatenate
    row_df = pd.DataFrame([row])
    df = pd.concat([df, row_df], ignore_index=True)
    return df

###############################################################################
# 3) CLASSIFIERS & PARAM DISTRIBUTIONS
###############################################################################
classifiers_and_params = {
    "kNN": (
        KNeighborsClassifier(),
        {
            "n_neighbors": randint(1, 50),
            "weights": ["uniform", "distance"],
            "p": [1, 2]
        }
    ),
    "SVM": (
        SVC(probability=True, random_state=42),
        {
            "C": uniform(0.01, 10),
            "kernel": ["linear", "rbf"],
            "gamma": ["scale", "auto"]
        }
    ),
    "LDA": (
        LinearDiscriminantAnalysis(),
        [
          {
              "solver": ["svd"],
              "shrinkage": [None],
              "tol": uniform(1e-5, 1e-2)
          },
          {
              "solver": ["lsqr","eigen"],
              "shrinkage": [None,"auto"],
              "tol": uniform(1e-5, 1e-2)
          }
        ]
    ),
    "LogReg": (
        LogisticRegression(random_state=42, max_iter=1000),
        {
            "C": uniform(0.01, 10),
            "penalty": ["l1","l2"],
            "solver": ["liblinear", "saga"]
        }
    ),
    "DecisionTree": (
        DecisionTreeClassifier(random_state=42),
        {
            "max_depth": randint(1, 20),
            "min_samples_split": randint(2, 10),
            "min_samples_leaf": randint(1, 10),
            "criterion": ["gini","entropy"]
        }
    ),
    "RandomForest": (
        RandomForestClassifier(random_state=42),
        {
            "n_estimators": randint(50, 300),
            "max_depth": randint(1, 20),
            "min_samples_split": randint(2, 10),
            "min_samples_leaf": randint(1, 10),
            "criterion": ["gini","entropy"]
        }
    ),
    "NaiveBayes": (
        GaussianNB(),
        {
            "var_smoothing": uniform(1e-9, 1e-5)
        }
    ),
    "AdaBoost": (
        AdaBoostClassifier(random_state=42),
        {
            "n_estimators": randint(50, 300),
            "learning_rate": uniform(0.01,0.5)
        }
    )
}

###############################################################################
# 4) OUTER CROSS-VALIDATION LOOP
###############################################################################
fold_number = 1
y = df['Bankruptcy_Status']
for train_index, test_index in skf.split(X_balanced, y_balanced):
    print(f"\n=== Fold {fold_number} ===")

    X_train_fold = X_balanced.iloc[train_index]
    X_test_fold  = X_balanced.iloc[test_index]
    y_train_fold = y_balanced.iloc[train_index]
    y_test_fold  = y_balanced.iloc[test_index]

    # For each classifier
    for clf_name, (clf_base, param_dist) in classifiers_and_params.items():
        print(f"\n... Working on {clf_name} for fold {fold_number} ...")

        # RandomizedSearchCV
        random_search = RandomizedSearchCV(
            estimator=clf_base,
            param_distributions=param_dist,
            n_iter=10,             # number of random combos
            scoring="f1",
            cv=3,                  # inner CV
            random_state=42,
            n_jobs=-1
        )

        # Fit on training fold
        random_search.fit(X_train_fold, y_train_fold)
        best_model = random_search.best_estimator_
        print(f"Best params for {clf_name}, fold {fold_number}:", random_search.best_params_)

        # Predict on train
        y_train_pred = best_model.predict(X_train_fold)
        y_train_prob = None
        if hasattr(best_model, "predict_proba"):
            y_train_prob = best_model.predict_proba(X_train_fold)[:, 1]

        # Predict on test
        y_test_pred = best_model.predict(X_test_fold)
        y_test_prob = None
        if hasattr(best_model, "predict_proba"):
            y_test_prob = best_model.predict_proba(X_test_fold)[:, 1]

        #######################################################################
        # (A) PRINTING & PLOTTING CONFUSION MATRICES + METRICS, as in the 1st code
        #######################################################################
        # Compute confusion matrices
        conf_matrix_train = confusion_matrix(y_train_fold, y_train_pred)
        conf_matrix_test  = confusion_matrix(y_test_fold,  y_test_pred)

        # Calculate main metrics
        train_accuracy  = accuracy_score(y_train_fold, y_train_pred)
        train_precision = precision_score(y_train_fold, y_train_pred, zero_division=0)
        train_recall    = recall_score(y_train_fold, y_train_pred, zero_division=0)
        train_f1        = f1_score(y_train_fold, y_train_pred, zero_division=0)
        train_auc       = roc_auc_score(y_train_fold, y_train_prob) if y_train_prob is not None else None

        test_accuracy   = accuracy_score(y_test_fold, y_test_pred)
        test_precision  = precision_score(y_test_fold, y_test_pred, zero_division=0)
        test_recall     = recall_score(y_test_fold, y_test_pred, zero_division=0)
        test_f1         = f1_score(y_test_fold, y_test_pred, zero_division=0)
        test_auc        = roc_auc_score(y_test_fold, y_test_prob) if y_test_prob is not None else None

        print("\n--- Training Set Metrics ---")
        print(f"Confusion Matrix (Train):\n{conf_matrix_train}")
        print(f"Accuracy:  {train_accuracy:.2f}")
        print(f"Precision: {train_precision:.2f}")
        print(f"Recall:    {train_recall:.2f}")
        print(f"F1 Score:  {train_f1:.2f}")
        if train_auc is not None:
            print(f"ROC-AUC:   {train_auc:.2f}")

        print("\n--- Test Set Metrics ---")
        print(f"Confusion Matrix (Test):\n{conf_matrix_test}")
        print(f"Accuracy:  {test_accuracy:.2f}")
        print(f"Precision: {test_precision:.2f}")
        print(f"Recall:    {test_recall:.2f}")
        print(f"F1 Score:  {test_f1:.2f}")
        if test_auc is not None:
            print(f"ROC-AUC:   {test_auc:.2f}")

        # Plot confusion matrices for train & test
        fig, ax = plt.subplots(1, 2, figsize=(14, 6))
        sns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', ax=ax[0])
        ax[0].set_title(f"{clf_name} - Fold {fold_number} (Train)")
        ax[0].set_xlabel("Predicted")
        ax[0].set_ylabel("Actual")

        sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', ax=ax[1])
        ax[1].set_title(f"{clf_name} - Fold {fold_number} (Test)")
        ax[1].set_xlabel("Predicted")
        ax[1].set_ylabel("Actual")

        plt.tight_layout()
        plt.show()

        # Plot ROC curves for train & test
        fig, ax = plt.subplots(1, 2, figsize=(14, 6))
        RocCurveDisplay.from_estimator(best_model, X_train_fold, y_train_fold, ax=ax[0])
        ax[0].set_title(f"{clf_name} - ROC (Train) - Fold {fold_number}")

        RocCurveDisplay.from_estimator(best_model, X_test_fold, y_test_fold, ax=ax[1])
        ax[1].set_title(f"{clf_name} - ROC (Test) - Fold {fold_number}")

        plt.tight_layout()
        plt.show()

        #######################################################################
        # (B) STORING METRICS in "fold_metrics"
        #######################################################################
        # We'll store only the test set metrics in fold_metrics for final summary
        fold_metrics["Classifier"].append(clf_name)
        fold_metrics["Fold"].append(fold_number)
        fold_metrics["Accuracy"].append(test_accuracy)
        fold_metrics["Precision"].append(test_precision)
        fold_metrics["Recall"].append(test_recall)
        fold_metrics["F1 Score"].append(test_f1)
        fold_metrics["AUC ROC"].append(test_auc if test_auc is not None else 0.0)

        #######################################################################
        # (C) LOGGING DETAILED ROWS TO "results_df"
        #######################################################################
        # TRAIN row
        results_df = log_metrics_to_df(
            results_df,
            classifier_name=clf_name,
            fold_number=fold_number,
            dataset_type="Train",
            balanced_status=balanced_status,
            y_true=y_train_fold,
            y_pred=y_train_pred,
            X_data=X_train_fold,
            y_data=y_train_fold,
            y_prob=y_train_prob
        )

        # TEST row
        results_df = log_metrics_to_df(
            results_df,
            classifier_name=clf_name,
            fold_number=fold_number,
            dataset_type="Test",
            balanced_status=balanced_status,
            y_true=y_test_fold,
            y_pred=y_test_pred,
            X_data=X_train_fold,    # or X_test_fold if you prefer
            y_data=y_train_fold,    # or y_test_fold
            y_prob=y_test_prob
        )

    fold_number += 1

###############################################################################
# 5) AFTER ALL FOLDS: PRINT SUMMARY & SAVE CSV
###############################################################################
# Summarize metrics across folds (test only) for each classifier
print("\n--- Summary of Metrics Across All Folds (Test Set) ---")
summary_df = pd.DataFrame(fold_metrics)
grouped = summary_df.groupby("Classifier")

for clf_name, group in grouped:
    mean_acc    = group["Accuracy"].mean()
    mean_prec   = group["Precision"].mean()
    mean_rec    = group["Recall"].mean()
    mean_f1     = group["F1 Score"].mean()
    mean_auc    = group["AUC ROC"].mean()
    print(f"\nClassifier: {clf_name}")
    print(f"  Accuracy (mean):  {mean_acc:.2f}")
    print(f"  Precision (mean): {mean_prec:.2f}")
    print(f"  Recall (mean):    {mean_rec:.2f}")
    print(f"  F1 (mean):        {mean_f1:.2f}")
    print(f"  AUC (mean):       {mean_auc:.2f}")

# Finally, write the detailed results_df to CSV
output_csv_file = "balancedDataOutcomes.csv"
results_df.to_csv(output_csv_file, index=False)
print(f"\nAll done! Detailed results saved to {output_csv_file}")